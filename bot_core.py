import asyncio
import logging
import os
import random
import re
from datetime import datetime

from aiogram import Bot
from dotenv import load_dotenv
from openai import OpenAI
import feedparser
import requests
from bs4 import BeautifulSoup

load_dotenv()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROUP_CHAT_ID = int(os.getenv("GROUP_CHAT_ID"))

client = OpenAI(api_key=OPENAI_API_KEY)
bot = Bot(token=TELEGRAM_TOKEN)

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0"}

async def generate_gpt_content(style="morning"):
    if style == "morning":
        prompt = (
            "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ü–µ–Ω–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è Telegram-–≥—Ä—É–ø–ø—ã –ø–æ –ò–ò, –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º—É –¥–∏–∑–∞–π–Ω—É –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.\n"
            "–§–æ—Ä–º–∞—Ç:\n"
            "–°–Ω–∞—á–∞–ª–∞ 1 –∫–æ—Ä–æ—Ç–∫–∏–π —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å —ç–º–æ–¥–∑–∏\n"
            "–ü–æ—Ç–æ–º 3‚Äì4 —Å–∞–º—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Å–≤–µ–∂–∏—Ö —Ñ–∞–∫—Ç–∞ –∏–ª–∏ –Ω–æ–≤–∏–Ω–∫–∏ (–∫–∞–∂–¥—ã–π —Ñ–∞–∫—Ç ‚Äî 1 —Å—Ç—Ä–æ–∫–∞, —Å —ç–º–æ–¥–∑–∏, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏, –±–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –±–µ–∑ –∑–≤—ë–∑–¥–æ—á–µ–∫)\n"
            "–ü–æ—Ç–æ–º 1‚Äì2 —Å–∏–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è (–∫–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å ‚Äî –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞)\n"
            "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π ** –∏–ª–∏ __ –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ‚Äî Telegram –∏—Ö –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –ü–∏—à–∏ –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –°—Ç–∏–ª—å: –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π, —Ü–µ–ø–ª—è—é—â–∏–π."
        )
    elif style == "afternoon":
        prompt = (
            "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ—Å—Ç (4‚Äì6 —Å—Ç—Ä–æ–∫) —Å 3‚Äì4 —Å–∞–º—ã–º–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–º–∏ –∏ –∫—Ä—É—Ç—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏ –ø–æ –ò–ò/ML/–¥–∏–∑–∞–π–Ω—É/–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.\n"
            "–ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç ‚Äî –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞, —Å —ç–º–æ–¥–∑–∏ –≤ –Ω–∞—á–∞–ª–µ, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏, –±–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –±–µ–∑ ** –∏ __.\n"
            "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –±–µ–∑ –≤–æ–ø—Ä–æ—Å–æ–≤. –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã. –°—Ç–∏–ª—å —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π –∏ —Ü–µ–ø–ª—è—é—â–∏–π."
        )
    else:
        prompt = "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 1 –æ—á–µ–Ω—å –∫—Ä—É—Ç–æ–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∏–Ω—Å–∞–π—Ç –ø–æ –ò–ò/ML/–¥–∏–∑–∞–π–Ω—É/–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å —ç–º–æ–¥–∑–∏)."

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.82,
        max_tokens=450
    )
    return response.choices[0].message.content.strip()


def get_news():
    feeds = [
        "https://arxiv.org/rss/cs.AI",
        "https://arxiv.org/rss/cs.LG",
        "https://arxiv.org/rss/cs.CV",
        "https://news.google.com/rss/search?q=–ò–ò+OR+–º–∞—à–∏–Ω–Ω–æ–µ+–æ–±—É—á–µ–Ω–∏–µ+OR+–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ&hl=ru&gl=RU&ceid=RU:ru",
    ]
    lines = []
    count = 0
    for url in feeds:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                if count >= 6:
                    break
                title = entry.title.replace("<", "&lt;").replace(">", "&gt;")
                link = entry.get("link", "#")
                lines.append(f"‚Ä¢ <a href='{link}'>{title}</a>")
                count += 1
            if count >= 6:
                break
        except:
            pass
    return "\n".join(lines) if lines else "<i>–Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å</i>"


def get_meme_urls():
    queries = ["programming meme", "AI meme funny", "coder humor", "developer meme", "machine learning joke"]
    all_images = []
    for q in random.sample(queries, k=3):
        try:
            url = f"https://www.google.com/search?q={q.replace(' ', '+')}&tbm=isch"
            r = requests.get(url, headers=HEADERS, timeout=10)
            if r.status_code != 200:
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            for img in soup.find_all("img"):
                src = img.get("src") or img.get("data-src") or ""
                if src.startswith("http") and re.search(r'\.(jpg|jpeg|png|gif)$', src, re.I):
                    all_images.append(src)
        except:
            pass
    all_images = list(set(all_images))[:8]
    random.shuffle(all_images)
    return all_images[:5]


async def morning_post():
    gpt = await generate_gpt_content("morning")
    news = get_news()
    text = f"{gpt}\n\nüì∞ {news}"
    await bot.send_message(GROUP_CHAT_ID, text, parse_mode="HTML", disable_web_page_preview=True)
    await bot.session.close()


async def afternoon_post():
    gpt = await generate_gpt_content("afternoon")
    await bot.send_message(GROUP_CHAT_ID, gpt, parse_mode="HTML")
    await bot.session.close()


async def evening_memes():
    urls = get_meme_urls()
    sent = 0
    for url in urls:
        if sent >= 4:
            break
        try:
            await bot.send_photo(GROUP_CHAT_ID, url, caption="üòÇ")
            sent += 1
            await asyncio.sleep(random.uniform(1.5, 3))
        except:
            pass
    if sent == 0:
        await bot.send_message(GROUP_CHAT_ID, "ü§ñ –º–µ–º–æ–≤ —Å–µ–≥–æ–¥–Ω—è –Ω–µ—Ç")
    await bot.session.close()